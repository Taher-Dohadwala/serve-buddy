# -*- coding: utf-8 -*-
"""serve-part-classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/158yHFfgFDarPL8AZDkvi83Jx5yVESBEu
"""

import tensorflow as tf
from tensorflow.keras.preprocessing import image
import os
import cv2
import numpy as np

from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2

d = "/tmp/train"
training_videos = [os.path.join(d,path) for path in os.listdir(d)]
print(training_videos)

# convert training videos into training images and labels

y = []
frames = []
for video_path in training_videos:
  print(f"Working on {video_path}")
  cap = cv2.VideoCapture(video_path)
  ret = True
  while ret:
    ret,img = cap.read()
    if ret:
      resized = cv2.resize(img,(224,224))
      frames.append(resized)
      y.append(int(video_path.split("-")[2].split(".")[0])-1)


X = np.array(frames)
y = np.array(y)

print(X.shape)
print(y.shape)

def normalize(image,label):
  image = tf.cast(image,tf.float32) / 255.0
  return image,label

train_dataset = tf.data.Dataset.from_tensor_slices((X,y))
train_dataset

train_dataset = (train_dataset
                 .map(normalize)
                 .shuffle(500)
                 .batch(32)
)

base_model = MobileNetV2(weights="imagenet",include_top=False)
base_model.trainable = False

# model = tf.keras.Sequential([
#       tf.keras.layers.Conv2D(32,kernel_size=5,input_shape=(224,224,3)),
#       tf.keras.layers.GlobalAveragePooling2D(),
#       tf.keras.layers.Flatten(),
#       tf.keras.layers.Dense(64,activation='relu'),
#       tf.keras.layers.Dense(8,activation="softmax",name="output")
# ])

model = tf.keras.Sequential([
      base_model,
      tf.keras.layers.GlobalAveragePooling2D(),
      tf.keras.layers.Flatten(),
      tf.keras.layers.Dense(64,activation='relu'),
      tf.keras.layers.Dense(8,activation="softmax",name="output")
])

model.compile(optimizer="adam",
              loss=tf.keras.losses.SparseCategoricalCrossentropy(),
              metrics = ["acc"]
)

history = model.fit(train_dataset,epochs=10)

model.save("taher_serve.h5")

